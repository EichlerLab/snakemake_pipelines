import pandas as pd


configfile: "config/config.yaml"


MANIFEST = config.get("manifest", "config/manifest.tab")
REFERENCE = config["reference"]
FLAGGER_FLAGS = config.get("flagger_flags", ["Col", "Err", "Dup", "Unk", "Hap"])
FLAGGER_FLAGS.append("all")
FLAGGER_FLAGS.append("nohap")
SUBGROUP_DICT = config.get("sample_subsets", {})
SUBGROUP_DICT["all"] = "all"
FLAGGER_LEN_FILTER = config.get("flagger_len_filter", 0)
manifest_df = pd.read_csv(MANIFEST, sep="\t", index_col=["SAMPLE"])


def find_paf(wildcards):
    # Concatenated h1 and h2 PAFs
    return manifest_df.at[wildcards.sample, "PAF"]


def find_flagger_bed(wildcards):
    return manifest_df.at[wildcards.sample, "FLAGGER_BED"]


def find_bed_per_sample(wildcards):
    if wildcards.subgroup == "all":
        return {z: manifest_df.loc[z]["FLAGGER_BED"] for z in manifest_df.index}
    else:
        return {
            z: manifest_df.loc[z]["FLAGGER_BED"]
            for z in SUBGROUP_DICT[wildcards.subgroup]
        }


def find_per_sample_flagger_summaries(wildcards):
    return expand(rules.summarize_bed_per_sample.output.tsv, sample=manifest_df.index)


def find_converted_pafs(wildcards):
    if wildcards.subgroup == "all":
        return expand(rules.liftover_coords.output.paf, sample=manifest_df.index)
    else:
        return expand(
            rules.liftover_coords.output.paf,
            sample=[x for x in SUBGROUP_DICT[wildcards.subgroup]],
        )


def ideo_ref_files(wildcards):
    return config["ref_files"][REFERENCE]


wildcard_constraints:
    sample="|".join(manifest_df.index),
    subgroup="|".join(SUBGROUP_DICT),
    ext="|".join(["png", "svg"]),
    flag="|".join(FLAGGER_FLAGS),


rule all:
    input:
        "results/flagger_summary/flagger_summary.tsv",
        expand(
            "results/plots/{subgroup}/{flag}-filt{filt}.{ext}",
            subgroup=SUBGROUP_DICT,
            flag=FLAGGER_FLAGS,
            ext=["png", "svg"],
            filt=FLAGGER_LEN_FILTER,
        ),
        expand(
            "results/flagger_summary/asm_bar_chart_{subgroup}-filt{filt}.png",
            subgroup=SUBGROUP_DICT,
            filt=FLAGGER_LEN_FILTER,
        ),
        expand(
            "results/flagger_summary/asm_bar_chart_nohap_{subgroup}-filt{filt}.png",
            subgroup=SUBGROUP_DICT,
            filt=FLAGGER_LEN_FILTER,
        ),
        expand("results/converted_bed/{sample}.bed", sample=manifest_df.index),



rule make_chain_for_liftover:
    input:
        paf=find_paf,
    output:
        chain_file="temp/chain/{sample}.chain"
    threads: 1
    singularity:
        "docker://eichlerlab/paf2chain:0.1.1"
    resources:
        mem=128,
        hrs=2,
    shell:
        """
        paf2chain -i {input.paf} > {output.chain_file}
        """

rule liftover_coords:
    input:
        chain_file=rules.make_chain_for_liftover.output.chain_file,
        flagger_bed=find_flagger_bed,
    output:
        new_bed="results/converted_paf/{sample}.bed",
        unmapped_bed="results/converted_paf/{sample}_unmapped.bed",
    threads: 1
    singularity:
         "docker://eichlerlab/ucsc:202405"
    resources:
        mem=128,
        hrs=2,
    shell:
        """
        liftOver <(cut -f 1-4 {input.flagger_bed} | tail -n +2 ) {input.chain_file} {output.new_bed} {output.unmapped_bed} 
        awk '{{s+=$3-$2;}} END {{print "Total Flagger bases before liftover: " s}}' {input.flagger_bed}
        awk '{{s+=$9-$8;}} END {{print "Total bases after liftover: " s}}' {output.new_bed}
        awk '{{s+=$9-$8;}} END {{print "Unmapped bases after liftover: " s}}' {output.unmapped_bed}
        """


rule liftover_bed:
    input:
        paf=rules.liftover_coords.output.new_bed,
    output:
        bed="results/converted_bed/{sample}.bed",
    threads: 1
    singularity:
         "docker://eichlerlab/rustybam:0.1.33"
    resources:
        mem=128,
        hrs=2,
    shell:
        """
        cut -f 1,3,4,13 {input.paf} | sed 's/id:Z://' | sed 's/_TO.*//' | awk '{{if (NF == 4) print}}' | sort -k1,1 -k2,2n > {output.bed}
        """


# Coordinate windows for make_ideogram binning
rule make_windows_bed:
    input:
        fai=config["ref_files"][REFERENCE]["fai"],
    output:
        bed="temp/windows.bed",
    threads: 1
    resources:
        mem=4,
        hrs=1,
    script:
        "../scripts/make_windows_bed.py"


rule make_ideogram:
    input:
        unpack(ideo_ref_files),
        paf=find_converted_pafs,
        bed=rules.make_windows_bed.output.bed,
    output:
        png="results/plots/{subgroup}/{flag}-filt{filt}.png",
        svg="results/plots/{subgroup}/{flag}-filt{filt}.svg",
    singularity:
        "docker://eichlerlab/ideoplot:0.1"
    threads: 1
    resources:
        mem=12,
        hrs=2,
    script:
        "../scripts/make_ideogram_flagger.py"


rule summarize_bed_per_sample:
    input:
        bed=find_flagger_bed,
    output:
        tsv="results/flagger_summary/per_sample/{sample}.tsv",
    threads: 1
    resources:
        mem=12,
        hrs=10,
    run:
        df = pd.read_csv(
            input.bed,
            sep="\t",
            header=0,
            usecols=[0, 1, 2, 3],
            names=["CHROM", "POS", "END", "ASM"],
        )
        df["Total"] = df["END"] - df["POS"]
        df = df.groupby("ASM").sum()[["Total"]].T
        df.insert(0, "Sample", f"{wildcards.sample}")
        df.to_csv(output.tsv, sep="\t", index=False)


rule make_bar_chart:
    input:
        unpack(find_bed_per_sample),
    output:
        tsv_all="results/flagger_summary/asm_bar_chart_{subgroup}-filt{filt}.png",
        tsv_nohap="results/flagger_summary/asm_bar_chart_nohap_{subgroup}-filt{filt}.png",
    threads: 1
    resources:
        mem=12,
        hrs=10,
    script:
        "../scripts/make_bar_chart.py"


rule combine_summaries:
    input:
        tsv=find_per_sample_flagger_summaries,
    output:
        tsv="results/flagger_summary/flagger_summary.tsv",
    threads: 1
    resources:
        mem=12,
        hrs=10,
    run:
        # Each sample is row in out table
        sample_dfs = [
            pd.read_csv(sample_summary, sep="\t") for sample_summary in input.tsv
        ]
        out_df = pd.concat(sample_dfs)

        out_df.to_csv(output.tsv, sep="\t", index=False)
